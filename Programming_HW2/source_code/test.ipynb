{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.utils.data as td\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_loaders(batch_size, shuffle_test=False):\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.225, 0.225, 0.225])\n",
    "    train = datasets.CIFAR10('~/data/cifar10', train=True, download=True,\n",
    "                             transform=transforms.Compose([\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.RandomCrop(32, 4),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 normalize,\n",
    "                             ]))\n",
    "    test = datasets.CIFAR10('~/data/cifar10', train=False,\n",
    "                            transform=transforms.Compose([transforms.ToTensor(), normalize]))\n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size,\n",
    "                                               shuffle=True, pin_memory=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size,\n",
    "                                              shuffle=shuffle_test, pin_memory=True)\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "train_loader, _ = cifar_loaders(batch_size)\n",
    "_, test_loader = cifar_loaders(test_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (image, labels) in enumerate(train_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor,\n",
       " torch.Size([64, 3, 32, 32]),\n",
       " tensor([5, 6, 3, 6, 5, 3, 4, 8, 2, 0, 2, 9, 4, 7, 5, 7, 1, 0, 5, 9, 6, 5, 3, 6,\n",
       "         1, 3, 2, 8, 4, 3, 9, 3, 1, 5, 7, 1, 5, 6, 2, 5, 5, 4, 9, 9, 8, 5, 5, 0,\n",
       "         3, 6, 7, 7, 0, 8, 0, 1, 8, 5, 5, 2, 1, 4, 3, 5]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(image), image.shape, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9f16a257f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_image = image[0]\n",
    "plt.imshow(sample_image.view(sample_image.shape[2], sample_image.shape[1], sample_image.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import MLP, mlp_preprocessing, CNN, cnn_preprocessing\n",
    "from tqdm import trange\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLP(3*32*32, 10)\n",
    "cnn = CNN(3*32*32, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3072])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_image = Variable(image.view(-1, 3*32*32))\n",
    "t_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([192, 1024])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.reshape(-1, 1024).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train batch 75:  10%|â–‰         | 76/782 [00:29<04:38,  2.54it/s, accuracy=0.0781, batch_loss=2.31, total_loss=2.3]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'Image'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/nr/_r872q0n30sfgln4hf3jw6_w0000gn/T/ipykernel_11707/2948058529.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mloss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0;31m# images = Variable(images.view(-1, 3*32*32)).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;31m# labels = labels.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    679\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 721\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.9/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \"\"\"\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/venv/lib/python3.9/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;31m# handle PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mmode_to_nptype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"I\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"I;16\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"F\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_to_nptype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"1\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'Image'"
     ]
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    'cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "opt = optim.Adam(mlp.parameters(), lr=5e-2)\n",
    "# with trange(3, desc=\"Training\", unit=\"epoch\") as bar:\n",
    "for epoch in range(1):\n",
    "    loss_history = []\n",
    "    with tqdm(train_loader) as bar:\n",
    "        for i, (images, labels) in enumerate(bar):\n",
    "            # images = Variable(images.view(-1, 3*32*32)).to(device)\n",
    "            # labels = labels.to(device)\n",
    "            images, labels = cnn_preprocessing(images, labels)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            ## Prediction\n",
    "            output = cnn(images)\n",
    "            loss = loss_fn(output, labels)\n",
    "            \n",
    "            predictions = output.argmax(dim=1).squeeze()\n",
    "            correct = (predictions == labels).sum().item()\n",
    "            accuracy = correct / images.shape[0]\n",
    "\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "            loss_history.append(loss.cpu().detach().numpy())\n",
    "            current_total_loss = np.mean(loss_history)\n",
    "            bar.set_description(f'Train batch {i}')\n",
    "            bar.set_postfix(batch_loss=loss.item(), total_loss=current_total_loss, accuracy=accuracy)\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.argmax(dim=1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "640cace4827df8bb8e1edb3566e4e979a7918609c95fe3eaf671877de41a29fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
